const END_URL = 'wss://nls-gateway.cn-shanghai.aliyuncs.com/ws/v1'
const APPKEY = 'ckxnZMvhbPN4jD6g' // è·å–Appkeyè¯·å‰å¾€æ§åˆ¶å°ï¼šhttps://nls-portal.console.aliyun.com/applist
// DEPRECATED const TOKEN = '5068bcef6c304c00aad17769f3944751' // è·å–Tokenå…·ä½“æ“ä½œï¼Œè¯·å‚è§ï¼šhttps://help.aliyun.com/document_detail/450514.html

const LOG_STYLES = {
  info: 'color: #8bb4f7',
  success: 'color: #67C23A',
  warning: 'color: #E6A23C',
  error: 'color: #F56C6C',
  highlight: 'color: #409EFF; font-weight: bold',
} as const

function log(message: string, style: keyof typeof LOG_STYLES = 'info') {
  console.log(`%c${message}`, LOG_STYLES[style])
}

export enum SpeechStatus {
  CONNECTING = 'Connecting',
  DISCONNECTED = 'Disconnected',
  CONNECTED = 'Connected',

  Ready = 'Ready',
  Listening = 'Listening',
  // Processing = 'Processing',
  End = 'End',

  Error = 'Error',
  Speaking = 'Speaking',
  Silent = 'Silent',
}

interface RootObject {
  header: Header
  payload: Payload
}

interface Payload {
  index: number
  time: number
  result: string
  confidence: number
  words: any[]
  status: number
  gender: string
  begin_time: number
  fixed_result: string
  unfixed_result: string
  stash_result: Stashresult
  audio_extra_info: string
  sentence_id: string
  gender_score: number
}

interface Stashresult {
  sentenceId: number
  beginTime: number
  text: string
  fixedText: string
  unfixedText: string
  currentTime: number
  words: any[]
}

interface Header {
  namespace: string
  name: string
  status: number
  message_id: string
  task_id: string
  status_text: string
}

export class SpeechNls {
  ws: WebSocket | undefined
  audioContext: AudioContext | undefined
  scriptProcessor: ScriptProcessorNode | undefined
  audioInput: MediaStreamAudioSourceNode | undefined
  // audioStream: MediaStream | undefined

  private token: string = ''
  cacheSentence: string = ''
  error: string = ''

  status: SpeechStatus = SpeechStatus.DISCONNECTED
  statusBus = useEventBus<SpeechStatus>('ON_STATUS_UPDATED')
  sentenceBus = useEventBus<Payload>('ON_SENTENCE')
  sentenceCacheBus = useEventBus<Payload>('ON_SENTENCE_CACHE')
  dataBus = useEventBus<RootObject>('ON_DATA')
  newConnectionStartBus = useEventBus<boolean>('ON_NEW_CONNECTION_START')

  private isSpeaking: boolean = false
  private silenceTimer: NodeJS.Timeout | null = null
  private readonly MAX_SILENCE_DURATION = 4000 // 4ç§’æ— å£°è‡ªåŠ¨æ–­å¼€
  private lastSpeechTime: number = Date.now()

  private speakingBus = useEventBus<boolean>('ON_SPEAKING_STATUS')
  private lastSpeakingState: boolean = false
  private isMonitoring: boolean = false

  // é…ç½®å‚æ•°
  private readonly SPEECH_THRESHOLD = 0.008 // å£°éŸ³æ£€æµ‹é˜ˆå€¼
  private volumeLevel: number = 0

  private animationFrameId: number | null = null
  private readonly LOG_OUTPUT_INTERVAL = 3000 // 3ç§’è¾“å‡ºä¸€æ¬¡æ—¥å¿—
  private lastLogTime: number = 0
  private isProcessingAudio: boolean = false

  private hasStartedTranscription: boolean = false
  private isConnecting: boolean = false

  private audioBuffer: Float32Array[] = [] // å­˜å‚¨éŸ³é¢‘æ•°æ®çš„ç¼“å†²åŒº
  private canSendAudio: boolean = false // æ§åˆ¶æ˜¯å¦å¯ä»¥å‘é€éŸ³é¢‘çš„æ ‡å¿—
  private speakingStartTime: number = 0 // å¼€å§‹è¯´è¯çš„æ—¶é—´æˆ³
  private readonly BUFFER_MAX_LENGTH = 2560 // é™åˆ¶ç¼“å†²åŒºå¤§å°
  private readonly MIN_SPEAKING_DURATION = 2000 // æœ€å°è¯´è¯æŒç»­æ—¶é—´ï¼ˆ2ç§’ï¼‰

  private currentUUID: string = ''

  getSpeakingStartTime() {
    return this.speakingStartTime
  }

  updateStatus(status: SpeechStatus) {
    this.status = status
    this.statusBus.emit(status)
  }

  async connect(token: string) {
    this.token = token
    this.error = ''
    this.updateStatus(SpeechStatus.CONNECTING)
    this.isMonitoring = true
    this.lastSpeechTime = Date.now()
    this.startConnection()
  }

  private reconnect() {
    if (this.token) {
      setTimeout(() => {
        // åªæœ‰åœ¨ä»ç„¶åœ¨ç›‘æµ‹çŠ¶æ€æ—¶æ‰é‡æ–°è¿æ¥
        if (this.isMonitoring)
          this.connect(this.token)
      }, 1200)
    }
  }

  startConnection() {
    if (!this.token)
      throw new Error('Token is not set')

    if (this.isConnecting)
      return

    this.currentUUID = generateUUID()

    this.isConnecting = true

    log('[NLS] å¼€å§‹å»ºç«‹WebSocketè¿æ¥...', 'highlight')
    const socketUrl = `${END_URL}?token=${this.token}`
    log(`[NLS] WebSocket URL: ${socketUrl}`, 'info')

    const websocket = this.ws = new WebSocket(socketUrl)

    websocket.onopen = () => {
      log('[NLS] âœ… WebSocketè¿æ¥å·²å»ºç«‹', 'success')
      this.updateStatus(SpeechStatus.CONNECTED)
      this.startSilenceDetection()
      this.sendStartTranscription()

      this.isConnecting = false
    }

    websocket.onmessage = (event) => {
      const message = JSON.parse(event.data) as RootObject
      this.dataBus.emit(message)

      // æ ¹æ®ä¸åŒæ¶ˆæ¯ç±»å‹è®°å½•æ—¥å¿—
      switch (message.header.name) {
        case 'TranscriptionStarted':
          log('[NLS] ğŸ¯ å¼€å§‹è¯­éŸ³è¯†åˆ«', 'success')
          this.updateStatus(SpeechStatus.Ready)
          this.newConnectionStartBus.emit(true)
          break
        case 'SentenceBegin':
          log('[NLS] ğŸ“ å¼€å§‹æ–°å¥å­', 'info')
          this.cacheSentence = ''
          this.sentenceCacheBus.emit(message.payload)
          break
        case 'TranscriptionResultChanged':
          log(`[NLS] ğŸ”„ å®æ—¶è¯†åˆ«ç»“æœ: ${message.payload.result}`, 'info')
          this.cacheSentence = message.payload.result
          this.sentenceCacheBus.emit(message.payload)
          break
        case 'SentenceEnd':
          log(`[NLS] âœ¨ å¥å­å®Œæˆ: ${message.payload.result}`, 'success')
          this.cacheSentence = message.payload.result
          this.sentenceCacheBus.emit(message.payload)
          this.sentenceBus.emit(message.payload)
          this.hasStartedTranscription = false // å¥å­ç»“æŸåé‡ç½®çŠ¶æ€
          break
        case 'TaskFailed':
          this.error = 'æ— æ³•å®Œæˆè¯†åˆ«'
          log(`[NLS] âŒ è¯†åˆ«ä»»åŠ¡å¤±è´¥: ${message.header.status_text}`, 'error')
          console.error('Task failed:', message)
          this.endConnection()
          break
        default:
          log(`[NLS] æ”¶åˆ°æ¶ˆæ¯: ${message.header.name}`, 'info')
      }
    }

    websocket.onerror = (event) => {
      this.error = 'æ— æ³•è¿æ¥è‡³è¿œç¨‹æœåŠ¡å™¨'
      log('[NLS] âŒ WebSocketè¿æ¥é”™è¯¯', 'error')
      console.error('WebSocket error:', event)
      this.updateStatus(SpeechStatus.Error)
      this.isMonitoring = false
      if (this.silenceTimer) {
        clearInterval(this.silenceTimer)
        this.silenceTimer = null
      }
    }

    websocket.onclose = () => {
      log('[NLS] WebSocketè¿æ¥å·²å…³é—­', 'warning')
      this.updateStatus(SpeechStatus.DISCONNECTED)
      this.currentUUID = ''
      this.hasStartedTranscription = false // é‡ç½®çŠ¶æ€
    }
  }

  private sendStartTranscription() {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN || this.hasStartedTranscription)
      return

    const startTranscriptionMessage = {
      header: {
        appkey: APPKEY,
        namespace: 'SpeechTranscriber',
        name: 'StartTranscription',
        task_id: this.currentUUID,
        message_id: this.currentUUID,
      },
      payload: {
        format: 'pcm',
        sample_rate: 16000,
        enable_intermediate_result: true,
        enable_punctuation_prediction: true,
        enable_inverse_text_normalization: true,
      },
    }

    log('[NLS] å‘é€å¼€å§‹è¯†åˆ«æŒ‡ä»¤', 'info')
    this.ws.send(JSON.stringify(startTranscriptionMessage))
    this.hasStartedTranscription = true
  }

  async endConnection() {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      log('[NLS] å‘é€åœæ­¢è¯†åˆ«æŒ‡ä»¤', 'warning')
      const stopTranscriptionMessage = {
        header: {
          appkey: APPKEY,
          namespace: 'SpeechTranscriber',
          name: 'StopTranscription',
          task_id: this.currentUUID,
          message_id: this.currentUUID,
        },
      }
      this.ws.send(JSON.stringify(stopTranscriptionMessage))
      this.hasStartedTranscription = false // é‡ç½®çŠ¶æ€
      log('[NLS] åœæ­¢è¯†åˆ«æŒ‡ä»¤å·²å‘é€', 'info')

      // ç»™ä¸€ä¸ª2så¤„ç†æ—¶é—´
      setTimeout(() => {
        this.ws?.close()
        log('[NLS] WebSocketè¿æ¥å·²å…³é—­', 'info')
      }, 2000)
    }
  }

  disconnect() {
    log('[NLS] å¼€å§‹æ–­å¼€è¿æ¥...', 'warning')
    this.endConnection()
    if (this.ws) {
      this.ws.close()
      log('[NLS] WebSocketè¿æ¥å·²å…³é—­', 'info')
    }
    this.token = ''
    this.isMonitoring = false
    log('[NLS] è¿æ¥å·²å®Œå…¨æ–­å¼€', 'success')
  }

  async startRecording(audioStream: MediaStream) {
    try {
      log('[NLS] å¼€å§‹å½•éŸ³å’ŒéŸ³é¢‘ç›‘æµ‹...', 'highlight')
      this.lastSpeechTime = Date.now()
      this.isMonitoring = true
      this.isProcessingAudio = true

      this.audioContext = new AudioContext({
        sampleRate: 16000,
      })
      this.audioInput = this.audioContext.createMediaStreamSource(audioStream)
      this.scriptProcessor = this.audioContext.createScriptProcessor(2048, 1, 1)

      this.scriptProcessor.onaudioprocess = (event) => {
        if (!this.isMonitoring)
          return

        const inputData = event.inputBuffer.getChannelData(0)
        const rms = Math.sqrt(inputData.reduce((acc, val) => acc + val * val, 0) / inputData.length)
        this.volumeLevel = rms
        const wasSpeaking = this.isSpeaking
        this.isSpeaking = rms > this.SPEECH_THRESHOLD

        // æ£€æµ‹è¯´è¯çŠ¶æ€å˜åŒ–
        if (this.isSpeaking && !wasSpeaking) {
          this.speakingStartTime = Date.now()
          this.canSendAudio = false
          this.audioBuffer = [] // æ¸…ç©ºç¼“å†²åŒº
        }

        // æ›´æ–°è¯´è¯çŠ¶æ€
        if (this.isSpeaking !== wasSpeaking) {
          const status = this.isSpeaking ? 'å¼€å§‹è¯´è¯' : 'åœæ­¢è¯´è¯'
          log(`[NLS] ğŸ¤ ${status} - éŸ³é‡: ${rms.toFixed(6)}`, this.isSpeaking ? 'success' : 'warning')
          this.speakingBus.emit(this.isSpeaking)
        }

        if (this.isSpeaking) {
          this.lastSpeechTime = Date.now()

          // åªè¦å¼€å§‹è¯´è¯ è¦ç¡®ä¿è¿æ¥åˆ°æœåŠ¡å™¨
          if (this.ws?.readyState !== WebSocket.OPEN)
            this.startConnection()

          // å­˜å‚¨éŸ³é¢‘æ•°æ®
          this.audioBuffer.push(new Float32Array(inputData))
          if (this.audioBuffer.length > this.BUFFER_MAX_LENGTH)
            this.audioBuffer.shift() // ç§»é™¤æœ€æ—§çš„æ•°æ®

          // æ£€æŸ¥æ˜¯å¦å·²ç»è¯´è¯è¶…è¿‡2ç§’
          if (!this.canSendAudio && Date.now() - this.speakingStartTime >= this.MIN_SPEAKING_DURATION) {
            this.canSendAudio = true
            // å‘é€ç¼“å†²åŒºä¸­çš„æ‰€æœ‰æ•°æ®
            this.sendBufferedAudio()
          }

          // å¦‚æœå¯ä»¥å‘é€ï¼Œä¸”WebSocketè¿æ¥æ­£å¸¸
          // this.canSendAudio &&
          if (this.ws?.readyState === WebSocket.OPEN) {
            const inputData16 = new Int16Array(inputData.length)
            for (let i = 0; i < inputData.length; ++i)
              inputData16[i] = Math.max(-1, Math.min(1, inputData[i])) * 0x7FFF

            this.ws.send(inputData16.buffer)
          }
        }
      }

      this.startVoiceCheck()
      this.audioInput.connect(this.scriptProcessor)
      this.scriptProcessor.connect(this.audioContext.destination)

      log('[NLS] âœ… å½•éŸ³åˆå§‹åŒ–æˆåŠŸ', 'success')
      this.updateStatus(SpeechStatus.Listening)
    }
    catch (e) {
      log('[NLS] âŒ å½•éŸ³å¯åŠ¨å¤±è´¥', 'error')
      console.error(e)
      throw e
    }
  }

  private startSilenceDetection() {
    // æ¸…é™¤ä¹‹å‰çš„è®¡æ—¶å™¨
    if (this.silenceTimer)
      clearInterval(this.silenceTimer)

    // æ¯ç§’æ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¶…è¿‡æ²‰é»˜æ—¶é—´
    this.silenceTimer = setInterval(() => {
      const silenceDuration = Date.now() - this.lastSpeechTime
      if (silenceDuration >= this.MAX_SILENCE_DURATION)
        this.handleLongSilence()
    }, 1000)
  }

  private async handleLongSilence() {
    if (this.silenceTimer) {
      clearInterval(this.silenceTimer)
      this.silenceTimer = null
    }

    try {
      await this.stopRecording()
      this.error = 'æ£€æµ‹åˆ°é•¿æ—¶é—´æ— äººè¯´è¯ï¼Œå·²è‡ªåŠ¨æ–­å¼€è¿æ¥'
      log('[NLS] âš ï¸ æ£€æµ‹åˆ°é•¿æ—¶é—´é™éŸ³ï¼Œè‡ªåŠ¨æ–­å¼€è¿æ¥', 'warning')
      this.updateStatus(SpeechStatus.Error)
    }
    catch (e) {
      log('[NLS] âŒ å¤„ç†é•¿æ—¶é—´é™éŸ³å¤±è´¥', 'error')
      console.error(e)
    }
  }

  private async handleStartSpeaking() {
    // å¦‚æœå½“å‰æœªè¿æ¥æˆ–å¤„äºé”™è¯¯çŠ¶æ€ï¼Œå°è¯•é‡æ–°è¿æ¥
    if (this.status === SpeechStatus.DISCONNECTED
      || this.status === SpeechStatus.Error) {
      try {
        if (this.token)
          await this.startConnection()
      }
      catch (e) {
        console.error('Failed to reconnect on speaking:', e)
      }
    }
  }

  private startVoiceCheck() {
    const checkVoice = () => {
      if (!this.isMonitoring) {
        this.animationFrameId = null
        return
      }

      const now = Date.now()
      // é™ä½æ™®é€šçŠ¶æ€æ—¥å¿—çš„è¾“å‡ºé¢‘ç‡åˆ°æ¯3ç§’ä¸€æ¬¡
      if (now - this.lastLogTime >= this.LOG_OUTPUT_INTERVAL) {
        const silenceDuration = ((now - this.lastSpeechTime) / 1000).toFixed(1)
        log(
          `[NLS] ç›‘æµ‹çŠ¶æ€ - éŸ³é‡: ${this.volumeLevel.toFixed(6)}, `
          + `çŠ¶æ€: ${this.isSpeaking ? 'è¯´è¯ä¸­' : 'é™éŸ³'}, `
          + `é™éŸ³æŒç»­: ${silenceDuration}s`,
          'info',
        )
        this.lastLogTime = now
      }

      this.animationFrameId = requestAnimationFrame(checkVoice)
    }

    checkVoice()
  }

  async stopRecording() {
    try {
      log('[NLS] åœæ­¢WebSocketè¿æ¥...', 'warning')
      await this.endConnection()

      log('[NLS] âœ… WebSocketè¿æ¥å·²å…³é—­ï¼Œç»§ç»­è¿›è¡ŒéŸ³é¢‘ç›‘æµ‹', 'success')
      this.updateStatus(SpeechStatus.End)
    }
    catch (e) {
      log('[NLS] âŒ åœæ­¢WebSocketè¿æ¥å¤±è´¥', 'error')
      console.error(e)
      throw e
    }
  }

  // å®Œå…¨åœæ­¢éŸ³é¢‘ç›‘æµ‹
  async stopAudioMonitoring() {
    this.isMonitoring = false
    this.isProcessingAudio = false

    if (this.animationFrameId !== null) {
      cancelAnimationFrame(this.animationFrameId)
      this.animationFrameId = null
    }

    if (this.silenceTimer) {
      clearInterval(this.silenceTimer)
      this.silenceTimer = null
    }

    if (this.scriptProcessor)
      this.scriptProcessor.disconnect()

    if (this.audioInput)
      this.audioInput.disconnect()

    if (this.audioContext)
      await this.audioContext.close()

    log('[NLS] âœ… éŸ³é¢‘ç›‘æµ‹å·²å®Œå…¨åœæ­¢', 'success')
  }

  // è·å–å½“å‰æ˜¯å¦æœ‰äººåœ¨è¯´è¯çš„çŠ¶æ€
  public getIsSpeaking(): boolean {
    return this.isSpeaking
  }

  // æ–°å¢ï¼šå‘é€ç¼“å†²åŒºä¸­çš„éŸ³é¢‘æ•°æ®
  private sendBufferedAudio() {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN)
      return

    log('[NLS] å¼€å§‹å‘é€ç¼“å­˜çš„éŸ³é¢‘æ•°æ®', 'info')

    for (const audioData of this.audioBuffer) {
      const inputData16 = new Int16Array(audioData.length)
      for (let i = 0; i < audioData.length; ++i)
        inputData16[i] = Math.max(-1, Math.min(1, audioData[i])) * 0x7FFF

      this.ws.send(inputData16.buffer)
    }

    this.audioBuffer = [] // æ¸…ç©ºç¼“å†²åŒº
  }

  /**
   * åˆ¤æ–­å½“å‰æ˜¯å¦æ»¡è¶³æ‰“æ–­æ¡ä»¶
   * @returns boolean æ˜¯å¦æ»¡è¶³æ‰“æ–­æ¡ä»¶
   */
  public checkInterruption(): boolean {
    // å¿…é¡»æ»¡è¶³æ¡ä»¶ï¼š
    // 1. å½“å‰æ­£åœ¨è¿›è¡Œè¯­éŸ³åˆæˆ
    // 2. å½“å‰éŸ³é‡è¶…è¿‡é˜ˆå€¼(æ­£åœ¨è¯´è¯)
    // 3. æŒç»­è¯´è¯è¶…è¿‡2ç§’
    const speakingDuration = this.isSpeaking ? Date.now() - this.speakingStartTime : 0
    const volumeAboveThreshold = this.volumeLevel > this.SPEECH_THRESHOLD

    return volumeAboveThreshold
      && speakingDuration >= 200
  }
}

function generateUUID(): string {
  return `${1e7}-${1e3}-${4e3}-${8e3}-${1e11}`.replace(/[018]/g, (c: string) =>
    (Number(c) ^ (crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> Number(c) / 4)).toString(16)).replace(/-/g, '')
}
